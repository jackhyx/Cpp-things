
### Mysql B+树
磁盘读写最小单位扇区 512B操作系统读写最小单位是块 Linux块 4 KB 8个块

* 二叉查找树：一个节点的左子树的所有节点都小于这个节点，右子树的所有节点都大于这个节点
* 每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成了一条链表，查找数据的时间复杂度变成了 O(n)
树的高度等于每次查询时的磁盘I0次数 高度越高，影响性能
退化成链表->On
插入元素越多，高度越高，IO次数越多，性能下降，不能范围查询

AVL树 自平衡二叉树 红黑树
在二叉搜索树的前提下，每个节点的左子树和右子树的高度差不能超过 1。O(logn)

都会随元素增多，树高度变高，磁盘I/0次数增多，影响效率

根本原因：都是二叉树
当树的节点越多的时候，并且树的分叉数 M 越大的时候，M 叉树的高度会远小于二叉树的高度。

B树 多叉树
但是 B 树的每个节点都包含数据（索引+记录），而用户的记录数据的大小很有可能远远超过了索引数据，这就需要花费更多的磁盘 I/O 操作次数来读到「有用的索引数据」。
而且，在我们查询位于底层的某个节点（比如 A 记录）过程中，「非 A 记录节点」里的记录数据会从磁盘加载到内存，但是这些记录数据是没用的，
我们只是想读取这些节点的索引数据来做比较查询，而「非 A 记录节点」里的记录数据对我们是没用的，这样不仅增多磁盘 I/O 操作次数，也占用内存资源。
范围查询需要中序遍历

B+ 树与 B 树差异的点，主要是以下这几点：

叶子节点才会存放实际数据（索引+记录），非叶子节点只会存放索引；

所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；

非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。

非叶子节点中有多少个子节点，就有多少个索引；

B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，
B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少

B+ 树有大量的冗余节点，这样使得删除一个节点的时候，可以直接从叶子节点中删除，甚至可以不动非叶子节点，这样删除非常快，
B 树则不同，B 树没有冗余节点，删除节点的时候非常复杂，比如删除根节点中的数据，可能涉及复杂的树的变形，

B+ 树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助
是 Innodb 使用的  B+ 树有一些特别的点，比如：

B+ 树的叶子节点之间是用「双向链表」进行连接，这样的好处是既能向右遍历，也能向左遍历。

B+ 树点节点内容是数据页，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是 16 KB

Innodb 根据索引类型不同，分为聚集和二级索引。他们区别在于，聚集索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚集索引的叶子节点，而二级索引的叶子节点存放的是主键值，而不是实际数据。
B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，
因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。

B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；

B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，
因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。

### Redis 数据结构
string--SDS
list -- 双向链表 压缩列表 -- quicklist
hash --  哈希表 压缩列表 -- listpack
set -- 哈希表 整数集合
Zset -- 跳表 压缩列表 -- listpack

SDS：保存文本 + 二进制 （数据存放在buf[])
O1获取长度 
API安全拼接不会造成缓冲区溢出（检查空间，自动扩容）
* 数据类型：
LIST: 元素个数小于512 值小于64字节 压缩列表 
否则双向链表
HASH: 元素个数小于512 值小于64字节 压缩列表
否则哈希表
SET:元素个数小于512 整数集合
否则哈希表
ZSET：元素个数小于128 值小于64 压缩列表
否则跳表

redisObject
type : 对象类型
encoding ： 使用那种底层数据结构
void* ptr
* 底层
#### SDS:
len 字符串长度
alloc 分配的空间长度 alloc - len 计算剩余空间
flags sds类型 5种类型
buf[] 字节数组 字符串 + 二进制
扩容：所需sds长度小于1M  2倍 newlen
超过1M newlen + 1M
节省空间：设计不同类型的结构体，灵活保存，节省空间
取消结构体对齐


#### LIST
list : head tail len -> listNode 
优点：
listNode : prev + next O1获取前置后置
list : head + tail O1获取表头和表尾
O1获取len
保存不同类型的值
缺点：内存不连续，无法很好利用CPU缓存，内存开销大


#### 压缩列表
优点：连续内存，利用CPU缓存，针对不同长度编码，节省内存
缺点：不能保存过多元素，降低查询效率；新增或修改时，内存重新分配，引发连锁更新

查找第一个元素和最后一个元素 O1
其他元素On
* 节点entry：
prevlen：前一个节点长度
enconding:当前实际数据的类型和长度
data：实际数据
会根据数据的类型和长度进行空间分配
前一个节点长度小于254 1字节空间分配prevlen字段
大于254 5字节空间分配prevlen字段
整数： encoding 1字节
字符串 ： 根据长度 125字节编码encoding
* 连锁更新问题

#### 哈希表
* 开链法解决哈希冲突
* rehash:两个哈希表：
给哈希表2分配比哈希表1大2倍的空间
哈希表1迁移至哈希表2
哈希表1空间释放，哈希表2->1 新建哈希表1为下次
* 渐进式rehash
* 触发条件：负载因子：已保存节点数/哈希表大小
大于等于1，且没有执行rdb快照和aof重写时触发
大于等于5，强制触发

#### 整数集合
整数集合的升级操作：节省内存
不支持降级操作

#### 跳表
* 平均logN查找
* 哈希表只是用于以常数复杂度获取元素权重
* 结构：多层有序链表
* 跳表节点查询过程 该层下一个节点 Or 下一层 level数组

查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。
* 在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：

如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。
如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。
如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。
* 跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O(logN)。

##### 那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？
如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。
Redis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。
具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），
那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。

#### 为什么用跳表而不用平衡树？ 内存 + 范围查找 + 算法实现
* 从内存占用上来比较，跳表比平衡树更灵活一些。
平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。
如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。

* 在做范围查找的时候，跳表比平衡树操作要简单。在平衡树上，我们找到指定范围的小值之后，
还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。
而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。
* 从算法实现难度上来比较，跳表比平衡树要简单得多。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速


#### quicklist 3.2 以后List
双向链表 + 压缩列表组合
本身是个链表，节点元素是压缩列表
* 通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能


#### listpack
它最大特点是 listpack 中每个节点不再包含前一个节点的长度了，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。

* 压缩列表的 entry 保存 prevlen 是为了实现节点从后往前遍历，知道前一个节点的长度，就可以计算前一个节点的偏移量。
* listpack 一样可以支持从后往前遍历的


### 缓存问题
* 缓存雪崩:大量缓存数据同时过期或者redis宕机
* 大量缓存数据同时过期
均匀设置过期时间，加上随机数
互斥锁：发现数据不在redis加上互斥锁，保证同一时间只有一个请求来构建缓存，同时加上过期时间
双key：备用key不会过期，更新时同时更新
缓存永久有效，交由后台进程定时更新

* 在业务刚上线的时候，我们最好提前把数据缓起来，而不是等待用户访问才来触发缓存构建，这就是所谓的缓存预热
* redis宕机
服务熔断：直接返回错误；减少对业务影响，请求限流机制
主从节点的方式构建 Redis 缓存高可靠集群。

* 缓存击穿：热点数据过期
互斥锁，热点数据不过期

* 缓存穿透即不再缓存也不再数据库
业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
黑客恶意攻击，故意大量访问某些读取不存在数据的业务；
API接口非法请求限制
缓存空值或默认值
我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在。

### 数据库和缓存一致性



### Redis 主从复制第一次同步
三个阶段：
* 建立连接， 协商同步
  从服务器：replicaof->psync:runID offset
  主：FULLRESYNC 全量复制

* 主服务器同步数据给从服务器
  主：bgsave:RDB 期间的写操作没有进来 造成主从服务器间数据不一致
  三个时间间隙写入replication buffer:
  主生成RDB
  主发送RDB给从
  从加载RDB

* 主服务器发送新写操作命令给从服务器
  从收到RDB 先清空 然后加载RDB
  主发送replication buffer 主从数据一致

### 基于长连接的命令传播

分摊主服务器的压力
两个方面 bgsave fork()子进程耗时阻塞主进程
传输RDB网络带宽
主服务器生成 RDB 和传输 RDB 的压力可以分摊到充当经理角色的从服务器。

### 增量复制： 网络断开又恢复
恢复网络后，从请求psync offset != -1
主：CONTINUE——> 发送从断线期间所执行的写命令
怎么知道发送那些增量数据：repl_backlog_buffer:默认1M 避免全量 应该尽可能大一些

从服务器要读取的数据还在里面 就增量复制——>写入replication buffer
不在了 就全量复制


### 怎么判断某个redis节点是否正常工作
心跳检测ping-pong 如果有一半以上节点Ping一个节点没有回应 集群就认为这个节点挂了

主：每个10s ping
从：每隔1s replconf ack{offset} 上报自身偏移量
实时检测主从节点网络状态；上报复制偏移量，检测复制数据是否丢失

#### 主从复制架构中，过期Key处理：
主节点处理或淘汰一个key后，模拟一条del命令发送给从

#### redis是同步复制还是异步复制
收到写命令后，先写到内部缓冲区，然后异步发送给从

#### replication buffer 、repl backlog buffer 区别如下：
repl backlog buffer：保存着最近传播的写命令。
出现的阶段不一样：
是在增量复制阶段出现，一个主节点只分配一个 repl backlog buffer；
在全量复制阶段和增量复制阶段都会出现，主节点会给每个新连接的从节点，分配一个 replication buffer；
这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样：
当 repl backlog buffer 满了，因为是环形结构，会直接覆盖起始位置数据;
当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，重新开始全量复制。

### 为什么会出现主从数据不一致？ 异步复制 无法强一致性

### 如何应对主从数据不一致：
尽量保证主从节点间网络连接状况良好
开发一个外部程序监控主从间复制进度

得到双方复制进度-> 双方复制进度差值-> 大于阈值 不让客户端和这个从节点进行数据读取， 减少读到不一致数据的情况

### Redis单线程为什么这么快？

大部分操作在内存中完成，并且采用高效的数据结构因此Redis的瓶颈可能是机器的内存或者网络带宽，而并非CPU

单线程模型避免多线程竞争，省去多线程切换的时间和性能开销，不会死锁

I/O多路复用，处理大量的客户端socket请求，一个Redis线程处理多个IO流

### 6.0引入多线程，提高处理网络IO的并行度，命令的执行还是单线程

### Redis持久化
AOF：每执行一条写命令，就把该命令以追加的方式写入到文件
RDB：某一时间内存数据，以二进制写入硬盘
混合持久化

### AOF写回
always 同步写回
everySec 每秒写回
No 由操作系统控制写回

AOF过大->AOF重写
读取数据库所有键值对，每个键值对用一条命令记录到新的AOF文件上->压缩了AOF文件的体积

AOF重写过程：后台进程bgrewriteaof

重写期间，主进程仍可以处理命令请求，不会阻塞主进程
父子进程共享内存，只读；任意一方修改了共享内存，发生写时复制，父子进程就有了独立数据副本，不用加锁

### 数据不一致？
* AOF重写缓冲区
  AOF重写期间，主进程三个工作
  执行客户端发来的命令
  将执行后写命令追加到AOF缓冲区
  将执行后写命令追加到AOF重写缓冲区

子进程完成AOF重写工作后，向主进程发送信号，信号处理函数
将AOF重写缓冲区的内容追加到新的AOF文件，保证数据一致
新的AOF文件改名，覆盖现有AOF